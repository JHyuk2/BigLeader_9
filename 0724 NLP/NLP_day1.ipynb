{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE5NNsFLrm2v"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1. tensorflow 설치\n",
        "pip install tensorflow\n",
        "\n",
        "2. import nltk\n",
        "nltk.download() # 자연어팩 설치\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QZPhGRmscAo",
        "outputId": "94dc2159-5e3a-486c-8bd7-d7372d5f5809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy"
      ],
      "metadata": {
        "id": "6YcJsDpWsd_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "konlpy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3YuIeyRxson2",
        "outputId": "83e3d3f1-b877-4d38-a2f8-30da16a334eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "3Te5ANqWsqZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "print(okt.morphs(u'단독입찰보다 복수입찰의 경우'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_VVe1yatD_B",
        "outputId": "39fcab6a-6486-4f97-b90a-12e05e259b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(u'아버지가방에들어가신다'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4c-ddDQtH23",
        "outputId": "08557b37-5541-4851-de38-b6f03d9703b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가방', '에', '들어가신다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "코퍼스(corpus) - 말뭉치\n",
        "단어가방(Bag of words?) - 단어들이 순서와 관계없이 저장된 사전\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1GG8FfZtNRM",
        "outputId": "f825b7f2-dc59-4376-c36a-dc0e3e45237d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n코퍼스(corpus) - 말뭉치\\n단어가방(Bag of words?) - 단어들이 순서와 관계없이 저장된 사전\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "WIjnrVSMvYGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha1kWAhrvpin",
        "outputId": "f69d959d-10c6-4894-d41d-7b6bc2675345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = \"Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in\"\n",
        "print(\"단어 토큰화 결과: \", word_tokenize(temp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwTjXetGvbL-",
        "outputId": "9f4ac1b5-90b6-4039-fdb6-70f3ad782bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 결과:  ['Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\"\n",
        "from nltk.tokenize import WordPunctTokenizer, sent_tokenize\n",
        "print(\"단어 토큰화 결과: \", sent_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8-Oidz3vnCS",
        "outputId": "f92046a1-344f-4668-f13c-637629591963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 결과:  ['Language is a thing of beauty.', 'But mastering a new language from scratch is quite a daunting prospect.', 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!', 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"단어 토큰화 결과: \", WordPunctTokenizer().tokenize(temp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skPWv6nxxl5s",
        "outputId": "00f06bdb-6fc6-4e56-97c1-8a64abeb2642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 결과:  ['Don', \"'\", 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4kR-WBEyoPu",
        "outputId": "d0bc777b-b19a-421d-c003-e6840a4cd69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "res = word_tokenize(temp)\n",
        "pos_tag(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyvFC0_cx7xz",
        "outputId": "1cfc4d17-742f-4e1b-bf5a-b2b47501f5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('And', 'CC'),\n",
              " ('that', 'IN'),\n",
              " ('’', 'NNP'),\n",
              " ('s', 'VBZ'),\n",
              " ('exactly', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('way', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('our', 'PRP$'),\n",
              " ('machines', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('In', 'IN'),\n",
              " ('order', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('computer', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('understand', 'VB'),\n",
              " ('any', 'DT'),\n",
              " ('text', 'NN'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('need', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('break', 'VB'),\n",
              " ('that', 'IN'),\n",
              " ('word', 'NN'),\n",
              " ('down', 'RP'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('way', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('our', 'PRP$'),\n",
              " ('machine', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('understand', 'VB'),\n",
              " ('.', '.'),\n",
              " ('That', 'DT'),\n",
              " ('’', 'VBZ'),\n",
              " ('s', 'NN'),\n",
              " ('where', 'WRB'),\n",
              " ('the', 'DT'),\n",
              " ('concept', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('tokenization', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('Processing', 'NNP'),\n",
              " ('(', '('),\n",
              " ('NLP', 'NNP'),\n",
              " (')', ')'),\n",
              " ('comes', 'VBZ'),\n",
              " ('in', 'IN')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILnIHAcJymLk",
        "outputId": "b0b55483-c3ec-44e8-fe10-c44b9f679dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.10/dist-packages (4.5.4)\n",
            "Requirement already satisfied: emoji==1.2.0 in /usr/local/lib/python3.10/dist-packages (from kss) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Requirement already satisfied: pecab in /usr/local/lib/python3.10/dist-packages (from kss) (1.0.8)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "# 한국어 문장 토큰화 도구"
      ],
      "metadata": {
        "id": "zQskEM5lF472"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"여름입니다. 날씨가 덥습니다! 딥러닝을 공부합니다. 네?\""
      ],
      "metadata": {
        "id": "gEG3z-4XGN7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kss.split_sentences(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFFmfo2DGRgC",
        "outputId": "97a1ce95-85f3-4602-960e-561e3c8765d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
            "For your information, Kss also supports mecab backend.\n",
            "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
            "Please refer to following web sites for details:\n",
            "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
            "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여름입니다.', '날씨가 덥습니다!', '딥러닝을 공부합니다.', '네?']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt, Kkma"
      ],
      "metadata": {
        "id": "IxhMwjReGsQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "Ll7rOPOGIju6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = okt.morphs(text)\n",
        "okt.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oH4ZVmxIqsW",
        "outputId": "6aeef2e7-91bb-42ee-8614-eb20b0d510dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('여름', 'Noun'),\n",
              " ('입니다', 'Adjective'),\n",
              " ('.', 'Punctuation'),\n",
              " ('날씨', 'Noun'),\n",
              " ('가', 'Josa'),\n",
              " ('덥습니다', 'Adjective'),\n",
              " ('!', 'Punctuation'),\n",
              " ('딥', 'Noun'),\n",
              " ('러닝', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('공부', 'Noun'),\n",
              " ('합니다', 'Verb'),\n",
              " ('.', 'Punctuation'),\n",
              " ('네', 'Noun'),\n",
              " ('?', 'Punctuation')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BevO8gVtI5Tg",
        "outputId": "d1499462-c067-4e6a-e790-0bbae77c4fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여름', '날씨', '딥', '러닝', '공부', '네']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orey3NN_ItVz",
        "outputId": "b8582510-f30d-4e6c-e63a-e3cb9a246943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('여름', 'NNG'),\n",
              " ('이', 'VCP'),\n",
              " ('ㅂ니다', 'EFN'),\n",
              " ('.', 'SF'),\n",
              " ('날씨', 'NNG'),\n",
              " ('가', 'JKS'),\n",
              " ('덥', 'VA'),\n",
              " ('습니다', 'EFN'),\n",
              " ('!', 'SF'),\n",
              " ('딥', 'NNG'),\n",
              " ('러닝', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('공부', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㅂ니다', 'EFN'),\n",
              " ('.', 'SF'),\n",
              " ('이', 'VCP'),\n",
              " ('네', 'EFN'),\n",
              " ('?', 'SF')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CghptgEsKVfN",
        "outputId": "1ee0eb7a-1707-43f3-aa3a-40d2d00b0eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여름',\n",
              " '이',\n",
              " 'ㅂ니다',\n",
              " '.',\n",
              " '날씨',\n",
              " '가',\n",
              " '덥',\n",
              " '습니다',\n",
              " '!',\n",
              " '딥',\n",
              " '러닝',\n",
              " '을',\n",
              " '공부',\n",
              " '하',\n",
              " 'ㅂ니다',\n",
              " '.',\n",
              " '이',\n",
              " '네',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTUmmJaIz8a",
        "outputId": "c25ca3ce-bc31-4e6e-dca8-830c42809a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('여름', 'NNG'),\n",
              " ('이', 'VCP'),\n",
              " ('ㅂ니다', 'EFN'),\n",
              " ('.', 'SF'),\n",
              " ('날씨', 'NNG'),\n",
              " ('가', 'JKS'),\n",
              " ('덥', 'VA'),\n",
              " ('습니다', 'EFN'),\n",
              " ('!', 'SF'),\n",
              " ('딥', 'NNG'),\n",
              " ('러닝', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('공부', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㅂ니다', 'EFN'),\n",
              " ('.', 'SF'),\n",
              " ('이', 'VCP'),\n",
              " ('네', 'EFN'),\n",
              " ('?', 'SF')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJgcFkbWKTR6",
        "outputId": "576f6afb-0a94-4647-aea4-3cf4d29cae15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여름', '날씨', '딥', '딥러닝', '러닝', '공부']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 낮거나, 길이가 매우 짧은 단어는 상황에 따라 제거 고려."
      ],
      "metadata": {
        "id": "OMb_JXcZKUG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pat = re.compile(r\"\\W*\\b\\w{1,2}\\b\")"
      ],
      "metadata": {
        "id": "e-7x2RpFKviE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규표현식을 사용하여 2글자 이하의 텍스트는 모두 제거.\n",
        "temp = \"Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in\"\n",
        "pat.sub('',temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "C3K9q3K2LNTj",
        "outputId": "31de20ed-5ba8-4e50-8645-6f5a63cb9600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-eh_xgGLWJl",
        "outputId": "98cd0221-2081-4a33-d428-93e495e89823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stopwords.words('english')\n",
        "\n",
        "wt = word_tokenize(temp)\n",
        "res = []\n",
        "for w in wt:\n",
        "  if w not in sw:\n",
        "    res.append(w)"
      ],
      "metadata": {
        "id": "U2qTenO5MVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wt)\n",
        "\n",
        "print(len(wt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChMf9zVgN-YU",
        "outputId": "bde633bf-3ace-460e-bf99-11c4f3f7c5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in']\n",
            "57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP를 열심히 공부하고, 취업에 성공합시다.\"\n",
        "sw = '를 에 고 라고 다'"
      ],
      "metadata": {
        "id": "yIdeAKCSOJbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = sw.split()\n",
        "sw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkpH5G9DOrc-",
        "outputId": "9781813b-b44f-414a-a747-ce68a9cac524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['를', '에', '고', '라고', '다']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wt = okt.morphs(text)\n",
        "\n",
        "tmp_list = [w for w in wt if not w in sw]\n",
        "tmp_list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qqsxcAyOu-A",
        "outputId": "9c170ac5-fbd6-4d43-a9b5-33833e6b2b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', '열심히', '공부', '하고', ',', '취업', '성공합시다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫인코딩\n",
        "# 단어 -> 정수\n",
        "\n",
        "text = \"\"\"\n",
        "Tokenization is a key (and mandatory) aspect of working with text data\n",
        "We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n",
        "Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n",
        "And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n",
        "Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R9kBYbeZO4em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = sent_tokenize(text)\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRofPCAvVJF4",
        "outputId": "94627621-d6ab-412d-8a7f-bf7c30512ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n",
              " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
              " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
              " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n",
              " 'And that’s exactly the way with our machines.',\n",
              " 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n",
              " 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n",
              " 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n",
              " 'Yes, it’s really that important!']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency를 확인\n",
        "vocab = {}\n",
        "pre_sents = []\n",
        "\n",
        "for s in sents:\n",
        "  wt = word_tokenize(s)\n",
        "  res = []\n",
        "\n",
        "  for w in wt:\n",
        "    w = w.lower()\n",
        "    if w not in sw:\n",
        "      if len(w) >2:\n",
        "        res.append(w)\n",
        "        if w not in vocab:\n",
        "          vocab[w] = 0\n",
        "        vocab[w] += 1\n",
        "  pre_sents.append(res)\n",
        "\n",
        "pre_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPUZdTKXVL66",
        "outputId": "f183d4d4-a033-44da-aaf7-fdcc971096eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['tokenization',\n",
              "  'key',\n",
              "  'and',\n",
              "  'mandatory',\n",
              "  'aspect',\n",
              "  'working',\n",
              "  'with',\n",
              "  'text',\n",
              "  'data',\n",
              "  'discuss',\n",
              "  'the',\n",
              "  'various',\n",
              "  'nuances',\n",
              "  'tokenization',\n",
              "  'including',\n",
              "  'how',\n",
              "  'handle',\n",
              "  'out-of-vocabulary',\n",
              "  'words',\n",
              "  'oov',\n",
              "  'language',\n",
              "  'thing',\n",
              "  'beauty'],\n",
              " ['but',\n",
              "  'mastering',\n",
              "  'new',\n",
              "  'language',\n",
              "  'from',\n",
              "  'scratch',\n",
              "  'quite',\n",
              "  'daunting',\n",
              "  'prospect'],\n",
              " ['you',\n",
              "  'ever',\n",
              "  'picked',\n",
              "  'language',\n",
              "  'that',\n",
              "  'wasn',\n",
              "  'your',\n",
              "  'mother',\n",
              "  'tongue',\n",
              "  'you',\n",
              "  'relate',\n",
              "  'this'],\n",
              " ['there',\n",
              "  'are',\n",
              "  'many',\n",
              "  'layers',\n",
              "  'peel',\n",
              "  'off',\n",
              "  'and',\n",
              "  'syntaxes',\n",
              "  'consider',\n",
              "  'quite',\n",
              "  'challenge'],\n",
              " ['and', 'that', 'exactly', 'the', 'way', 'with', 'our', 'machines'],\n",
              " ['order',\n",
              "  'get',\n",
              "  'our',\n",
              "  'computer',\n",
              "  'understand',\n",
              "  'any',\n",
              "  'text',\n",
              "  'need',\n",
              "  'break',\n",
              "  'that',\n",
              "  'word',\n",
              "  'down',\n",
              "  'way',\n",
              "  'that',\n",
              "  'our',\n",
              "  'machine',\n",
              "  'can',\n",
              "  'understand'],\n",
              " ['that',\n",
              "  'where',\n",
              "  'the',\n",
              "  'concept',\n",
              "  'tokenization',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'nlp',\n",
              "  'comes'],\n",
              " ['simply',\n",
              "  'put',\n",
              "  'can',\n",
              "  'work',\n",
              "  'with',\n",
              "  'text',\n",
              "  'data',\n",
              "  'don',\n",
              "  'perform',\n",
              "  'tokenization'],\n",
              " ['yes', 'really', 'that', 'important']]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['and']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps_iArebWfcI",
        "outputId": "8377fa7b-1210-4704-9725-e4a2242d1f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vs = list(sorted(vocab.items(), key=lambda x:x[1], reverse=True))\n",
        "vs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu2WD41WWsas",
        "outputId": "66c51535-14ea-4ca1-8196-d0521da92856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('that', 6),\n",
              " ('tokenization', 4),\n",
              " ('language', 4),\n",
              " ('and', 3),\n",
              " ('with', 3),\n",
              " ('text', 3),\n",
              " ('the', 3),\n",
              " ('our', 3),\n",
              " ('data', 2),\n",
              " ('quite', 2),\n",
              " ('you', 2),\n",
              " ('way', 2),\n",
              " ('understand', 2),\n",
              " ('can', 2),\n",
              " ('key', 1),\n",
              " ('mandatory', 1),\n",
              " ('aspect', 1),\n",
              " ('working', 1),\n",
              " ('discuss', 1),\n",
              " ('various', 1),\n",
              " ('nuances', 1),\n",
              " ('including', 1),\n",
              " ('how', 1),\n",
              " ('handle', 1),\n",
              " ('out-of-vocabulary', 1),\n",
              " ('words', 1),\n",
              " ('oov', 1),\n",
              " ('thing', 1),\n",
              " ('beauty', 1),\n",
              " ('but', 1),\n",
              " ('mastering', 1),\n",
              " ('new', 1),\n",
              " ('from', 1),\n",
              " ('scratch', 1),\n",
              " ('daunting', 1),\n",
              " ('prospect', 1),\n",
              " ('ever', 1),\n",
              " ('picked', 1),\n",
              " ('wasn', 1),\n",
              " ('your', 1),\n",
              " ('mother', 1),\n",
              " ('tongue', 1),\n",
              " ('relate', 1),\n",
              " ('this', 1),\n",
              " ('there', 1),\n",
              " ('are', 1),\n",
              " ('many', 1),\n",
              " ('layers', 1),\n",
              " ('peel', 1),\n",
              " ('off', 1),\n",
              " ('syntaxes', 1),\n",
              " ('consider', 1),\n",
              " ('challenge', 1),\n",
              " ('exactly', 1),\n",
              " ('machines', 1),\n",
              " ('order', 1),\n",
              " ('get', 1),\n",
              " ('computer', 1),\n",
              " ('any', 1),\n",
              " ('need', 1),\n",
              " ('break', 1),\n",
              " ('word', 1),\n",
              " ('down', 1),\n",
              " ('machine', 1),\n",
              " ('where', 1),\n",
              " ('concept', 1),\n",
              " ('natural', 1),\n",
              " ('processing', 1),\n",
              " ('nlp', 1),\n",
              " ('comes', 1),\n",
              " ('simply', 1),\n",
              " ('put', 1),\n",
              " ('work', 1),\n",
              " ('don', 1),\n",
              " ('perform', 1),\n",
              " ('yes', 1),\n",
              " ('really', 1),\n",
              " ('important', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list = [[3*i + j+1 for j in range(3)] for i in range(3)]\n",
        "temp_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK_Punt3Zqok",
        "outputId": "d6b6591f-0b9a-46b7-981d-cd628bfd209a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_transpose = list(zip(*temp_list[::-1]))\n",
        "temp_transpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_Qm5JajbEw5",
        "outputId": "4aea6d64-a3c6-4bd0-ff95-d5706be841f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7, 4, 1), (8, 5, 2), (9, 6, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[[7, 4, 1],\n",
        " [8, 5, 2],\n",
        " [9, 6, 3]\n",
        " ]"
      ],
      "metadata": {
        "id": "qsy_EGe8bMDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "HsAYiOLcglvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "temp = 'aabbcccdddd'\n",
        "\n",
        "cnt = Counter(temp)\n",
        "cnt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-p1dXhNgoy8",
        "outputId": "31aa601c-4cbb-4be9-a12c-604a24d24bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'a': 2, 'b': 2, 'c': 3, 'd': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index={}\n",
        "a=0\n",
        "for w, f in vs:\n",
        "    if f>=2:\n",
        "        a+=1\n",
        "        word_index[w]=a\n",
        "\n",
        "word_index['OOV'] = len(word_index) + 1 # out of vocab\n",
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG33eRBWg77b",
        "outputId": "a1d088bc-baf7-41a4-e47e-ad83ea68e2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'that': 1,\n",
              " 'tokenization': 2,\n",
              " 'language': 3,\n",
              " 'and': 4,\n",
              " 'with': 5,\n",
              " 'text': 6,\n",
              " 'the': 7,\n",
              " 'our': 8,\n",
              " 'data': 9,\n",
              " 'quite': 10,\n",
              " 'you': 11,\n",
              " 'way': 12,\n",
              " 'understand': 13,\n",
              " 'can': 14,\n",
              " 'OOV': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sents=[]\n",
        "for s in pre_sents:\n",
        "    enc_sent=[]\n",
        "    for w in s:\n",
        "        try:\n",
        "            enc_sent.append(word_index[w])\n",
        "        except KeyError:\n",
        "            enc_sent.append(word_index['OOV'])\n",
        "    enc_sents.append(enc_sent)\n",
        "enc_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJmUQ3rojrOT",
        "outputId": "b3145034-1e38-40bf-f2ba-32531b8371ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2,\n",
              "  15,\n",
              "  4,\n",
              "  15,\n",
              "  15,\n",
              "  15,\n",
              "  5,\n",
              "  6,\n",
              "  9,\n",
              "  15,\n",
              "  7,\n",
              "  15,\n",
              "  15,\n",
              "  2,\n",
              "  15,\n",
              "  15,\n",
              "  15,\n",
              "  15,\n",
              "  15,\n",
              "  15,\n",
              "  3,\n",
              "  15,\n",
              "  15],\n",
              " [15, 15, 15, 3, 15, 15, 10, 15, 15],\n",
              " [11, 15, 15, 3, 1, 15, 15, 15, 15, 11, 15, 15],\n",
              " [15, 15, 15, 15, 15, 15, 4, 15, 15, 10, 15],\n",
              " [4, 1, 15, 7, 12, 5, 8, 15],\n",
              " [15, 15, 8, 15, 13, 15, 6, 15, 15, 1, 15, 15, 12, 1, 8, 15, 14, 13],\n",
              " [1, 15, 7, 15, 2, 15, 3, 15, 15, 15],\n",
              " [15, 15, 14, 15, 5, 6, 9, 15, 15, 2],\n",
              " [15, 15, 1, 15]]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "데이터수집(크롤링) 도구 : selenium, beatifulsoup4 ...\n",
        "시각화 도구: matplotlib, plotly, tableau, seaborn, folium\n",
        "데이터분석도구 : numpy, pandas\n",
        "머신러닝도구 : scikit-learn\n",
        "딥러닝 프레임워크 : tensorflow(keras), pytorch, cafe ...\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "_iQgFrwhkbrX",
        "outputId": "77b7a6d7-70bc-4ac6-b516-2d98874d8ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n데이터수집(크롤링) 도구 : selenium, beatifulsoup4 ...\\n시각화 도구: matplotlib, plotly, tableau, seaborn, folium\\n데이터분석도구 : numpy, pandas\\n머신러닝도구 : scikit-learn\\n딥러닝 프레임워크 : tensorflow(keras), pytorch, cafe ...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "dYgOxkdRlyGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEFxZ_Cyl4e9",
        "outputId": "beb9b6fa-8f8a-4b0f-f97e-dbb607a5a873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['tokenization',\n",
              "  'key',\n",
              "  'and',\n",
              "  'mandatory',\n",
              "  'aspect',\n",
              "  'working',\n",
              "  'with',\n",
              "  'text',\n",
              "  'data',\n",
              "  'discuss',\n",
              "  'the',\n",
              "  'various',\n",
              "  'nuances',\n",
              "  'tokenization',\n",
              "  'including',\n",
              "  'how',\n",
              "  'handle',\n",
              "  'out-of-vocabulary',\n",
              "  'words',\n",
              "  'oov',\n",
              "  'language',\n",
              "  'thing',\n",
              "  'beauty'],\n",
              " ['but',\n",
              "  'mastering',\n",
              "  'new',\n",
              "  'language',\n",
              "  'from',\n",
              "  'scratch',\n",
              "  'quite',\n",
              "  'daunting',\n",
              "  'prospect'],\n",
              " ['you',\n",
              "  'ever',\n",
              "  'picked',\n",
              "  'language',\n",
              "  'that',\n",
              "  'wasn',\n",
              "  'your',\n",
              "  'mother',\n",
              "  'tongue',\n",
              "  'you',\n",
              "  'relate',\n",
              "  'this'],\n",
              " ['there',\n",
              "  'are',\n",
              "  'many',\n",
              "  'layers',\n",
              "  'peel',\n",
              "  'off',\n",
              "  'and',\n",
              "  'syntaxes',\n",
              "  'consider',\n",
              "  'quite',\n",
              "  'challenge'],\n",
              " ['and', 'that', 'exactly', 'the', 'way', 'with', 'our', 'machines'],\n",
              " ['order',\n",
              "  'get',\n",
              "  'our',\n",
              "  'computer',\n",
              "  'understand',\n",
              "  'any',\n",
              "  'text',\n",
              "  'need',\n",
              "  'break',\n",
              "  'that',\n",
              "  'word',\n",
              "  'down',\n",
              "  'way',\n",
              "  'that',\n",
              "  'our',\n",
              "  'machine',\n",
              "  'can',\n",
              "  'understand'],\n",
              " ['that',\n",
              "  'where',\n",
              "  'the',\n",
              "  'concept',\n",
              "  'tokenization',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'nlp',\n",
              "  'comes'],\n",
              " ['simply',\n",
              "  'put',\n",
              "  'can',\n",
              "  'work',\n",
              "  'with',\n",
              "  'text',\n",
              "  'data',\n",
              "  'don',\n",
              "  'perform',\n",
              "  'tokenization'],\n",
              " ['yes', 'really', 'that', 'important']]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(pre_sents)"
      ],
      "metadata": {
        "id": "KCbcXFU6mabA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m2sbxVpmUFW",
        "outputId": "9fab8d0d-e101-44df-fe1e-56fe77180b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'that': 1,\n",
              " 'tokenization': 2,\n",
              " 'language': 3,\n",
              " 'and': 4,\n",
              " 'with': 5,\n",
              " 'text': 6,\n",
              " 'the': 7,\n",
              " 'our': 8,\n",
              " 'data': 9,\n",
              " 'quite': 10,\n",
              " 'you': 11,\n",
              " 'way': 12,\n",
              " 'understand': 13,\n",
              " 'can': 14,\n",
              " 'key': 15,\n",
              " 'mandatory': 16,\n",
              " 'aspect': 17,\n",
              " 'working': 18,\n",
              " 'discuss': 19,\n",
              " 'various': 20,\n",
              " 'nuances': 21,\n",
              " 'including': 22,\n",
              " 'how': 23,\n",
              " 'handle': 24,\n",
              " 'out-of-vocabulary': 25,\n",
              " 'words': 26,\n",
              " 'oov': 27,\n",
              " 'thing': 28,\n",
              " 'beauty': 29,\n",
              " 'but': 30,\n",
              " 'mastering': 31,\n",
              " 'new': 32,\n",
              " 'from': 33,\n",
              " 'scratch': 34,\n",
              " 'daunting': 35,\n",
              " 'prospect': 36,\n",
              " 'ever': 37,\n",
              " 'picked': 38,\n",
              " 'wasn': 39,\n",
              " 'your': 40,\n",
              " 'mother': 41,\n",
              " 'tongue': 42,\n",
              " 'relate': 43,\n",
              " 'this': 44,\n",
              " 'there': 45,\n",
              " 'are': 46,\n",
              " 'many': 47,\n",
              " 'layers': 48,\n",
              " 'peel': 49,\n",
              " 'off': 50,\n",
              " 'syntaxes': 51,\n",
              " 'consider': 52,\n",
              " 'challenge': 53,\n",
              " 'exactly': 54,\n",
              " 'machines': 55,\n",
              " 'order': 56,\n",
              " 'get': 57,\n",
              " 'computer': 58,\n",
              " 'any': 59,\n",
              " 'need': 60,\n",
              " 'break': 61,\n",
              " 'word': 62,\n",
              " 'down': 63,\n",
              " 'machine': 64,\n",
              " 'where': 65,\n",
              " 'concept': 66,\n",
              " 'natural': 67,\n",
              " 'processing': 68,\n",
              " 'nlp': 69,\n",
              " 'comes': 70,\n",
              " 'simply': 71,\n",
              " 'put': 72,\n",
              " 'work': 73,\n",
              " 'don': 74,\n",
              " 'perform': 75,\n",
              " 'yes': 76,\n",
              " 'really': 77,\n",
              " 'important': 78}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YnL4m7dmgJs",
        "outputId": "802f7770-b0fc-4ff0-cccd-bf224e2746ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('tokenization', 4),\n",
              "             ('key', 1),\n",
              "             ('and', 3),\n",
              "             ('mandatory', 1),\n",
              "             ('aspect', 1),\n",
              "             ('working', 1),\n",
              "             ('with', 3),\n",
              "             ('text', 3),\n",
              "             ('data', 2),\n",
              "             ('discuss', 1),\n",
              "             ('the', 3),\n",
              "             ('various', 1),\n",
              "             ('nuances', 1),\n",
              "             ('including', 1),\n",
              "             ('how', 1),\n",
              "             ('handle', 1),\n",
              "             ('out-of-vocabulary', 1),\n",
              "             ('words', 1),\n",
              "             ('oov', 1),\n",
              "             ('language', 4),\n",
              "             ('thing', 1),\n",
              "             ('beauty', 1),\n",
              "             ('but', 1),\n",
              "             ('mastering', 1),\n",
              "             ('new', 1),\n",
              "             ('from', 1),\n",
              "             ('scratch', 1),\n",
              "             ('quite', 2),\n",
              "             ('daunting', 1),\n",
              "             ('prospect', 1),\n",
              "             ('you', 2),\n",
              "             ('ever', 1),\n",
              "             ('picked', 1),\n",
              "             ('that', 6),\n",
              "             ('wasn', 1),\n",
              "             ('your', 1),\n",
              "             ('mother', 1),\n",
              "             ('tongue', 1),\n",
              "             ('relate', 1),\n",
              "             ('this', 1),\n",
              "             ('there', 1),\n",
              "             ('are', 1),\n",
              "             ('many', 1),\n",
              "             ('layers', 1),\n",
              "             ('peel', 1),\n",
              "             ('off', 1),\n",
              "             ('syntaxes', 1),\n",
              "             ('consider', 1),\n",
              "             ('challenge', 1),\n",
              "             ('exactly', 1),\n",
              "             ('way', 2),\n",
              "             ('our', 3),\n",
              "             ('machines', 1),\n",
              "             ('order', 1),\n",
              "             ('get', 1),\n",
              "             ('computer', 1),\n",
              "             ('understand', 2),\n",
              "             ('any', 1),\n",
              "             ('need', 1),\n",
              "             ('break', 1),\n",
              "             ('word', 1),\n",
              "             ('down', 1),\n",
              "             ('machine', 1),\n",
              "             ('can', 2),\n",
              "             ('where', 1),\n",
              "             ('concept', 1),\n",
              "             ('natural', 1),\n",
              "             ('processing', 1),\n",
              "             ('nlp', 1),\n",
              "             ('comes', 1),\n",
              "             ('simply', 1),\n",
              "             ('put', 1),\n",
              "             ('work', 1),\n",
              "             ('don', 1),\n",
              "             ('perform', 1),\n",
              "             ('yes', 1),\n",
              "             ('really', 1),\n",
              "             ('important', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩\n",
        "pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"
      ],
      "metadata": {
        "id": "KWtDOmmNnQHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 정수 인코딩\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(pre_sentences) # 모든 단어에 대해 번호를 부여"
      ],
      "metadata": {
        "id": "MAnDt0m2n07n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tok.texts_to_sequences(pre_sentences) # 텍스트가 정수 시퀀스로 바뀐다."
      ],
      "metadata": {
        "id": "R6OzkTcGoUyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.index_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa7QqlSzoop5",
        "outputId": "611d7416-bb28-4607-aa23-bbd811cdb082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'bad',\n",
              " 2: 'driver',\n",
              " 3: 'person',\n",
              " 4: 'huge',\n",
              " 5: 'good',\n",
              " 6: 'knew',\n",
              " 7: 'kept'}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩?\n",
        "# LSTM에 넣기 위해서는 각 문장의 길이를 통일시켜줘야 한다.\n",
        "# 가장 길이가 긴 sentence의 길이를 찾은 후 0값을 넣어준다.\n",
        "maxlen = max(len(i) for i in encoded)"
      ],
      "metadata": {
        "id": "EIE-GbWMorqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in encoded:\n",
        "  while (len(s)) < maxlen:\n",
        "    s.append(0)\n",
        "  print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTEGLhonp6cx",
        "outputId": "331dfcd2-6354-4de3-e890-3b7c8c85c393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 0, 0]\n",
            "[2, 5, 3, 0]\n",
            "[2, 4, 3, 0]\n",
            "[6, 1, 0, 0]\n",
            "[1, 7, 4, 1]\n",
            "[4, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rGYd6xxqPv4",
        "outputId": "b2d0e910-0ed8-4cb0-b009-0229859b8c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 0, 0],\n",
              " [2, 5, 3, 0],\n",
              " [2, 4, 3, 0],\n",
              " [6, 1, 0, 0],\n",
              " [1, 7, 4, 1],\n",
              " [4, 1, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = np.array(encoded)\n",
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCd6J2cqc3E",
        "outputId": "6aee6daa-2b14-4caa-b02a-ac63e0c3e362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3, 0, 0],\n",
              "       [2, 5, 3, 0],\n",
              "       [2, 4, 3, 0],\n",
              "       [6, 1, 0, 0],\n",
              "       [1, 7, 4, 1],\n",
              "       [4, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스 도구를 이용한 패딩\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # 많이 사용됨."
      ],
      "metadata": {
        "id": "AdQyG-dXqpdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded2 = tok.texts_to_sequences(pre_sentences)\n",
        "encoded2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WlbGUa7rFtE",
        "outputId": "dc081fe0-b558-4f19-9c28-3e4643b0c815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded2)\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ6y0tuWrQz7",
        "outputId": "55993826-beac-4260-bcc3-7db6ac6a4b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 2, 3],\n",
              "       [0, 2, 5, 3],\n",
              "       [0, 2, 4, 3],\n",
              "       [0, 0, 6, 1],\n",
              "       [1, 7, 4, 1],\n",
              "       [0, 0, 4, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded2, padding='post', maxlen=10) # padding = pre가 기본값\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ise6yybTr4zY",
        "outputId": "8573c329-955c-460c-9831-b65295943737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [2, 5, 3, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [2, 4, 3, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 7, 4, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [4, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(encoded, maxlen=2, truncating='post') # 0이 아닌 숫자들을 길이2로 자르기.\n",
        "padded"
      ],
      "metadata": {
        "id": "Whyy-wA1sgdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [0]\n",
        "temp2 = [i for i in range(100000)]"
      ],
      "metadata": {
        "id": "uSFyuxeIrYrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "temp2.insert(0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNqTJD6Nrqqi",
        "outputId": "220bbc25-73aa-4a16-e179-5edf67f15c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89.1 µs ± 26.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "queue = deque([temp2])"
      ],
      "metadata": {
        "id": "ESLblFt5woAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "queue.appendleft(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Echkdg0wu9W",
        "outputId": "d0239d02-72da-4d8d-9a9f-b308651bc4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91.7 ns ± 22.2 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫인코딩\n",
        "# 단어 종류 : 10개 => 인코딩 => 머신러닝 / 딥러닝 언어모델링(분류, 생성, 이해...)\n",
        "# 0~9번까지 번호 부여(0:sky, 1:coumputer)\n",
        "\n",
        "tokens = okt.morphs(\"자연어처리 공부를 합니다.\")\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7YFOyo6sMq1",
        "outputId": "507fede0-11b8-41fe-a1e2-bf84dbe9463d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['자연어', '처리', '공부', '를', '합니다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스\n",
        "text=\"점심 메뉴로 소고기볶음밥을 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.\""
      ],
      "metadata": {
        "id": "iQWgl57aypOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "QhcUzXMgzG8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU6w5Kc_zN6p",
        "outputId": "727657aa-3a83-410d-fc57-97aff6763019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'점심': 1,\n",
              " '메뉴로': 2,\n",
              " '소고기볶음밥을': 3,\n",
              " '먹었습니다': 4,\n",
              " '소고기볶음밥': 5,\n",
              " '너무': 6,\n",
              " '맛있어요': 7,\n",
              " '또': 8,\n",
              " '먹을래요': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=\"내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.\""
      ],
      "metadata": {
        "id": "Zft3E8jjzPAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tok.texts_to_sequences([test])[0]\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85TBcW7DzsCp",
        "outputId": "9a361491-8d3b-4003-ffca-618067f3466b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 5, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "56Ib5pbCzuYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o_v = to_categorical(encoded)\n",
        "o_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9IrTB-f0vAO",
        "outputId": "0165b341-3f3a-4928-b5f4-aa713238ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 홍길동 영화 => 추천?\n",
        "# 영화의 줄거리를 기반으로 추천(tf-idf => cosine sim => 추)"
      ],
      "metadata": {
        "id": "3tOmhSJX0yrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "temp = 'aaaabbbcccdddd'\n",
        "cnt = Counter(temp)\n",
        "print(cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ood7Ufoy-kH",
        "outputId": "31c52b88-a327-4a30-e47e-673ebb5c78bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'a': 4, 'd': 4, 'b': 3, 'c': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxnWwkY8zIzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}